{"cells":[{"cell_type":"markdown","id":"6d4226ed-389a-4105-81ec-45f84d201283","metadata":{"id":"6d4226ed-389a-4105-81ec-45f84d201283"},"source":["# Final model"]},{"cell_type":"code","execution_count":null,"id":"4b6824fd-78a8-48e7-b61f-de252b2b47f0","metadata":{"id":"4b6824fd-78a8-48e7-b61f-de252b2b47f0"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/My Drive/Colab Notebooks/AN2DL/Homework1"]},{"cell_type":"markdown","id":"e6fe1c3f-92a3-401b-824a-3566ed99645e","metadata":{"id":"e6fe1c3f-92a3-401b-824a-3566ed99645e"},"source":["## Import libraries"]},{"cell_type":"code","execution_count":null,"id":"a1d76c2e-8caa-4b01-ae84-ca2b6c3c7037","metadata":{"id":"a1d76c2e-8caa-4b01-ae84-ca2b6c3c7037"},"outputs":[],"source":["import random\n","import os\n","from pathlib import Path\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","plt.style.use('ggplot')\n","\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n","from sklearn.utils import class_weight\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import layers\n","\n","print(tf.__version__)"]},{"cell_type":"markdown","id":"016e0285-676a-4e31-a2dc-108656d75c66","metadata":{"id":"016e0285-676a-4e31-a2dc-108656d75c66"},"source":["# Define auxiliary functions"]},{"cell_type":"code","execution_count":null,"id":"70bfcd01-898a-4030-89a9-edd6df9a6e36","metadata":{"id":"70bfcd01-898a-4030-89a9-edd6df9a6e36"},"outputs":[],"source":["def nice_plot(hist):\n","    \"\"\"Prints the diagnostic plot\"\"\"\n","    fig, (ax1,ax2) = plt.subplots(2, sharex=True) # Two subplots, the axes array is 1-d\n","    \n","    fig.suptitle('Training and validation')\n","\n","    ax1.plot(hist.history['accuracy'], label='Training accuracy')\n","    ax1.plot(hist.history['val_accuracy'], label='Validation accuracy')\n","    #ax1.set_title('Training and validation accuracy')\n","    ax1.set_ylabel('Accuracy')\n","    ax1.set_xlabel('Epochs')\n","    ax1.legend(loc='upper left')\n","    \n","    ax2.plot(history.history['loss'], label='Training loss')\n","    ax2.plot(history.history['val_loss'], label='Validation loss')\n","    #ax2.set_title('Training and validation loss')\n","    ax2.set_ylabel('Loss')\n","    ax2.set_xlabel('Epochs')\n","    ax2.legend(loc='upper left')\n","    \n","    plt.show()\n","    \n","\n","def evaluate_model(model, dataset):\n","    \"\"\"Prints the accuracy of the model evaluated on the given dataset\"\"\"\n","    test_loss, test_acc = model.evaluate(dataset)\n","    print(f'Accuracy: {test_acc*100:.2f}%')\n","    \n","\n","def print_trainability(model):\n","    \"\"\"Show which layers are trainable and which are not\"\"\"\n","    for i, layer in enumerate(model.layers):\n","        print(f'{i:<2} {layer.name:<13} {layer.trainable}')\n","\n","\n","def compile_model(model, lr=1e-3, opt='Adam'):\n","    \"\"\"Compile model according to learning rate and optimizer with CategoricalCrossentropy loss\"\"\"\n","    if opt == 'Adam':\n","        chosen_opt = keras.optimizers.Adam(learning_rate=lr)\n","    elif opt == 'RMSprop':\n","        chosen_opt = keras.optimizers.RMSprop(learning_rate=lr)\n","    else:\n","        print('No valid optimizer chosen')\n","        return\n","    model.compile(\n","        loss=keras.losses.CategoricalCrossentropy(),\n","        optimizer=chosen_opt,\n","        metrics=['accuracy'])\n","\n","\n","def fit_model(model, patience=20, append_callback=None, weights=None):\n","    \"\"\"Fit model with EarlyStopping on validation accuracy and class weights if specified\"\"\"\n","    callbacks = [\n","        keras.callbacks.EarlyStopping(\n","            monitor='val_accuracy',\n","            mode='max',\n","            patience=patience,\n","            restore_best_weights=True),\n","    ]\n","\n","    if append_callback:\n","        callbacks.append(append_callback)\n","\n","    if weights:\n","        history = model.fit(train_dataset,epochs=5000,validation_data=validation_dataset,callbacks=callbacks,class_weight=weights)\n","    else:\n","        history = model.fit(train_dataset,epochs=5000,validation_data=validation_dataset,callbacks=callbacks)\n","    \n","    return history"]},{"cell_type":"markdown","id":"f059ca75-4da8-4647-a06f-4ba6d7e33b03","metadata":{"tags":[],"id":"f059ca75-4da8-4647-a06f-4ba6d7e33b03"},"source":["# Global settings"]},{"cell_type":"code","execution_count":null,"id":"db75c688-78a5-4a6f-adc5-3503132c01f4","metadata":{"id":"db75c688-78a5-4a6f-adc5-3503132c01f4"},"outputs":[],"source":["IMAGE_SHAPE = [96,96]\n","INPUT_SHAPE = (*IMAGE_SHAPE,3)\n","BATCH_SIZE = 16\n","SEED = 42\n","DATASET_DIR = Path() / 'training_data_final'\n","MODELS_DIR = Path() / 'models'\n","MODELS_DIR.mkdir(parents=True, exist_ok=True)\n","CLASSES = [f'Species{i+1}' for i in range(8)]\n","NUM_CLASSES = len(CLASSES)"]},{"cell_type":"code","execution_count":null,"id":"c8f66834-8a81-4a62-afe6-8b0c83525855","metadata":{"id":"c8f66834-8a81-4a62-afe6-8b0c83525855"},"outputs":[],"source":["tf.random.set_seed(SEED)"]},{"cell_type":"markdown","id":"737ec175-b1d2-4d56-8e1b-d16f0b998282","metadata":{"id":"737ec175-b1d2-4d56-8e1b-d16f0b998282"},"source":["# Instantiate the dataset generators\n","\n","Create two generators, one for training and one for validation.\n","It should be more correct to create one generator only and select `subset='training'` and `subset='validation'`, but in this way augmentation is applied to both sets. I prefer to create [two generators with the same split and the same seed.](https://stackoverflow.com/questions/71744605/keras-imagedatagenerator-validation-split-does-not-split-validation-data-as-expe)"]},{"cell_type":"code","execution_count":null,"id":"cc5afaec-6fce-46d9-b368-9f6d949f4cc1","metadata":{"id":"cc5afaec-6fce-46d9-b368-9f6d949f4cc1"},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# train generator with augmentation\n","train_image_gen  = ImageDataGenerator(rotation_range=40,\n","                                      width_shift_range=0.2,\n","                                      height_shift_range=0.2,\n","                                      zoom_range=[0.5,1.5],\n","                                      brightness_range=[0.5,1.5],\n","                                      shear_range=0.2,\n","                                      vertical_flip=True,\n","                                      horizontal_flip=True,\n","                                      fill_mode='reflect',\n","                                      validation_split = 0.15,\n","                                      )\n","\n","# validation generator without augmentation\n","validation_image_gen = ImageDataGenerator(validation_split = 0.15)\n","\n","train_dataset = train_image_gen.flow_from_directory(directory=DATASET_DIR,\n","                                                    target_size=IMAGE_SHAPE,\n","                                                    color_mode='rgb',\n","                                                    classes=None,\n","                                                    class_mode='categorical',\n","                                                    batch_size=BATCH_SIZE,\n","                                                    shuffle=True,\n","                                                    seed=SEED,\n","                                                    subset='training',\n","                                                    )\n","\n","validation_dataset = validation_image_gen.flow_from_directory(directory=DATASET_DIR,\n","                                                              target_size=IMAGE_SHAPE,\n","                                                              color_mode='rgb',\n","                                                              classes=None,\n","                                                              class_mode='categorical',\n","                                                              batch_size=BATCH_SIZE,\n","                                                              shuffle=False,\n","                                                              seed=SEED,\n","                                                              subset='validation'\n","                                                              )"]},{"cell_type":"markdown","id":"f37a0700-5916-49f1-ae0e-fced4fd92ba1","metadata":{"id":"f37a0700-5916-49f1-ae0e-fced4fd92ba1"},"source":["#Â Vanilla CNN baseline"]},{"cell_type":"code","execution_count":null,"id":"07250d90-e7ef-4f0b-99af-da2c86929eab","metadata":{"id":"07250d90-e7ef-4f0b-99af-da2c86929eab"},"outputs":[],"source":["train_dataset_not_augmented = validation_image_gen.flow_from_directory(directory=DATASET_DIR,\n","                                                                       target_size=IMAGE_SHAPE,\n","                                                                       color_mode='rgb',\n","                                                                       classes=None,\n","                                                                       class_mode='categorical',\n","                                                                       batch_size=BATCH_SIZE,\n","                                                                       shuffle=True,\n","                                                                       seed=SEED,\n","                                                                       subset='training',\n","                                                                       )"]},{"cell_type":"code","execution_count":null,"id":"d1728bab-b432-4fb6-9925-271668064f32","metadata":{"id":"d1728bab-b432-4fb6-9925-271668064f32"},"outputs":[],"source":["inputs = keras.Input(shape=INPUT_SHAPE)\n","x = layers.Rescaling(1./255)(inputs)\n","\n","for size in [32, 64, 128, 256, 512]:\n","    x = layers.Conv2D(size, 3, padding='same', activation='relu', kernel_initializer=keras.initializers.HeUniform(seed=SEED))(x)\n","    x = layers.MaxPooling2D(2)(x)\n","\n","x = layers.Flatten()(x)\n","x = layers.Dropout(0.3)(x)\n","x = layers.Dense(512, kernel_initializer=keras.initializers.HeUniform(seed=SEED), activation='relu')(x)\n","x = layers.Dropout(0.3)(x)\n","outputs = layers.Dense(NUM_CLASSES, activation='softmax', kernel_initializer=keras.initializers.GlorotUniform())(x)\n","\n","# Connect input and output through the Model class\n","model_baseline = keras.Model(inputs, outputs, name='vanilla_CNN')\n","\n","# Compile the model\n","compile_model(model_baseline, lr=1e-3, opt='Adam')"]},{"cell_type":"code","execution_count":null,"id":"2186316d-d6b8-4e95-90b4-1b1f6f2f71c2","metadata":{"id":"2186316d-d6b8-4e95-90b4-1b1f6f2f71c2"},"outputs":[],"source":["history = model_baseline.fit(\n","    x = train_dataset_not_augmented,\n","    epochs = 5000,\n","    validation_data = validation_dataset,\n","    callbacks = keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=15, restore_best_weights=True)\n",")"]},{"cell_type":"code","execution_count":null,"id":"4a7da528-490a-424a-8c49-4f4b698a7696","metadata":{"id":"4a7da528-490a-424a-8c49-4f4b698a7696"},"outputs":[],"source":["evaluate_model(model_baseline, validation_dataset)\n","nice_plot(history)"]},{"cell_type":"markdown","id":"6df53751-9c5e-4ae7-9c81-36b63e6f0d5b","metadata":{"id":"6df53751-9c5e-4ae7-9c81-36b63e6f0d5b"},"source":["# Supernet comparison\n","\n","In the separate `supernet_choice.ipynb` notebook for clarity.\n","\n","# Keras tuner\n","\n","In the separate `keras_tuner.ipynb` notebook for clarity."]},{"cell_type":"markdown","id":"cccc8a5e-69bc-4019-9965-9979c17a19e9","metadata":{"id":"cccc8a5e-69bc-4019-9965-9979c17a19e9"},"source":["# Compute the class weights to account for class imbalance\n","\n","Class weights are computed [according to this.](https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras)"]},{"cell_type":"code","execution_count":null,"id":"f1c9a69c-af2a-4c4e-8c01-f09a92284cfa","metadata":{"id":"f1c9a69c-af2a-4c4e-8c01-f09a92284cfa"},"outputs":[],"source":["class_weights = class_weight.compute_class_weight(class_weight='balanced',\n","                                                  classes=np.unique(train_dataset.classes),\n","                                                  y=train_dataset.classes)\n","\n","class_weights = dict(zip(np.unique(train_dataset.classes), class_weights))\n","class_weights"]},{"cell_type":"markdown","id":"80acf92b-d97c-47bc-985f-de2c9c7149ce","metadata":{"id":"80acf92b-d97c-47bc-985f-de2c9c7149ce"},"source":["# Training VGG16"]},{"cell_type":"code","execution_count":null,"id":"30eeb1ba-7557-4f9c-9922-e6718a16fbe9","metadata":{"id":"30eeb1ba-7557-4f9c-9922-e6718a16fbe9"},"outputs":[],"source":["MODELS_DIR_VGG = MODELS_DIR / 'model_vgg16'\n","MODELS_DIR_VGG.mkdir(parents=True, exist_ok=True)"]},{"cell_type":"markdown","id":"9038d9fd-7221-4e85-9dc2-114413aea402","metadata":{"id":"9038d9fd-7221-4e85-9dc2-114413aea402"},"source":["## Instantiate supernet"]},{"cell_type":"code","execution_count":null,"id":"bb1271df-276c-46ca-a7b3-a1943e3870ea","metadata":{"id":"bb1271df-276c-46ca-a7b3-a1943e3870ea"},"outputs":[],"source":["conv_base  = keras.applications.VGG16(\n","    weights='imagenet',\n","    include_top=False,\n","    input_shape=(224, 224,3)\n",")\n","conv_base.trainable = False"]},{"cell_type":"markdown","id":"e6ef3e22-3ada-4741-8439-69c91d5ffd35","metadata":{"id":"e6ef3e22-3ada-4741-8439-69c91d5ffd35"},"source":["## Build model"]},{"cell_type":"code","execution_count":null,"id":"203e8513-eb49-4f3e-ab92-acec1db1cffb","metadata":{"id":"203e8513-eb49-4f3e-ab92-acec1db1cffb"},"outputs":[],"source":["inputs = keras.Input(shape=INPUT_SHAPE)\n","\n","x = layers.Resizing(224, 224, interpolation='bicubic', name='resizing')(inputs)\n","x = keras.applications.vgg16.preprocess_input(x)\n","x = conv_base(x)\n","\n","x = layers.GlobalAveragePooling2D()(x)\n","\n","x = layers.Dense(384, kernel_initializer=keras.initializers.HeUniform(seed=SEED))(x)\n","x = layers.BatchNormalization()(x)\n","x = layers.Activation('relu')(x)\n","x = layers.Dropout(0.3)(x)\n","\n","x = layers.Dense(256, kernel_initializer=keras.initializers.HeUniform(seed=SEED))(x)\n","x = layers.BatchNormalization()(x)\n","x = layers.Activation('relu')(x)\n","x = layers.Dropout(0.3)(x)\n","\n","outputs = layers.Dense(NUM_CLASSES, activation='softmax', kernel_initializer=keras.initializers.GlorotUniform(seed=SEED))(x)\n","\n","model_vgg16 = keras.Model(inputs, outputs, name='vgg16')\n","\n","compile_model(model_vgg16, lr=1e-3, opt='Adam')"]},{"cell_type":"markdown","id":"916a9ef5-d6d9-4d63-9b5f-ec8ed29c5ee9","metadata":{"id":"916a9ef5-d6d9-4d63-9b5f-ec8ed29c5ee9"},"source":["## Perform transfer learning"]},{"cell_type":"code","execution_count":null,"id":"8001d2bd-55f0-47bf-b07d-f10fa6add840","metadata":{"id":"8001d2bd-55f0-47bf-b07d-f10fa6add840"},"outputs":[],"source":["history = fit_model(model_vgg16, patience=20, weights=class_weights)\n","evaluate_model(model_vgg16, validation_dataset)\n","nice_plot(history)"]},{"cell_type":"code","execution_count":null,"id":"7d40889b-45a6-41c3-9d5e-a0b3d0c4586e","metadata":{"id":"7d40889b-45a6-41c3-9d5e-a0b3d0c4586e"},"outputs":[],"source":["model_vgg16.save(MODELS_DIR_VGG / '01_no_finetuning')"]},{"cell_type":"markdown","id":"05bef5ca-4b2e-4c7e-b5b8-4bbcc13f606f","metadata":{"id":"05bef5ca-4b2e-4c7e-b5b8-4bbcc13f606f"},"source":["## Fine tuning (1st pass)"]},{"cell_type":"code","execution_count":null,"id":"b4e5bc7f-f519-4947-b503-9a42d7671085","metadata":{"id":"b4e5bc7f-f519-4947-b503-9a42d7671085"},"outputs":[],"source":["DEFROST = 4"]},{"cell_type":"code","execution_count":null,"id":"a79ac624-f6a4-4f60-9b53-742597dfdcf3","metadata":{"id":"a79ac624-f6a4-4f60-9b53-742597dfdcf3"},"outputs":[],"source":["model_vgg16 = keras.models.load_model(MODELS_DIR_VGG / '01_no_finetuning')\n","\n","model_vgg16.get_layer('vgg16').trainable = True\n","for i, layer in enumerate(model_vgg16.get_layer('vgg16').layers[:-DEFROST]):\n","    layer.trainable = False\n","\n","compile_model(model_vgg16, lr=1e-4, opt='Adam')"]},{"cell_type":"code","execution_count":null,"id":"cb8434ec-9437-4d84-beb3-f96e3b9c096b","metadata":{"id":"cb8434ec-9437-4d84-beb3-f96e3b9c096b"},"outputs":[],"source":["history = fit_model(model_vgg16, patience=20)\n","evaluate_model(model_vgg16, validation_dataset)\n","nice_plot(history)"]},{"cell_type":"code","execution_count":null,"id":"57efef9b-2add-4838-9b3e-471dc29112ca","metadata":{"id":"57efef9b-2add-4838-9b3e-471dc29112ca"},"outputs":[],"source":["model_vgg16.save(MODELS_DIR_VGG / '02_finetuning_pass1')"]},{"cell_type":"markdown","id":"65c68d44-53f9-46d9-a8ee-1e5a4eb5f240","metadata":{"id":"65c68d44-53f9-46d9-a8ee-1e5a4eb5f240"},"source":["## Fine tuning (2nd pass)"]},{"cell_type":"code","execution_count":null,"id":"cbef3c48-a080-433d-b31a-d421a9bd8aab","metadata":{"id":"cbef3c48-a080-433d-b31a-d421a9bd8aab"},"outputs":[],"source":["DEFROST = 8"]},{"cell_type":"code","execution_count":null,"id":"73506536-f5f5-4cf5-9d57-51c3392e3421","metadata":{"id":"73506536-f5f5-4cf5-9d57-51c3392e3421"},"outputs":[],"source":["model_vgg16 = keras.models.load_model(MODELS_DIR_VGG / '02_finetuning_pass1')\n","\n","model_vgg16.get_layer('vgg16').trainable = True\n","for i, layer in enumerate(model_vgg16.get_layer('vgg16').layers[:-DEFROST]):\n","    layer.trainable = False\n","\n","compile_model(model_vgg16, lr=1e-4, opt='Adam')"]},{"cell_type":"code","execution_count":null,"id":"fb384d72-0c38-466c-b6f0-525c2ea91c50","metadata":{"id":"fb384d72-0c38-466c-b6f0-525c2ea91c50"},"outputs":[],"source":["append_callback = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-6)\n","history = fit_model(model_vgg16, patience=20, append_callback=append_callback)\n","evaluate_model(model_vgg16, validation_dataset)\n","nice_plot(history)"]},{"cell_type":"code","execution_count":null,"id":"85e42548-ef3e-4454-88d8-13a9bcc7d43c","metadata":{"id":"85e42548-ef3e-4454-88d8-13a9bcc7d43c"},"outputs":[],"source":["model_vgg16.save(MODELS_DIR_VGG / '03_finetuning_pass2')"]},{"cell_type":"markdown","id":"18707ec6-f8e9-454e-ae94-dfba55164631","metadata":{"id":"18707ec6-f8e9-454e-ae94-dfba55164631"},"source":["## Fine tuning (3rd pass)"]},{"cell_type":"code","execution_count":null,"id":"33175ab8-65f5-44c8-bdcf-efed59cbcb8e","metadata":{"id":"33175ab8-65f5-44c8-bdcf-efed59cbcb8e"},"outputs":[],"source":["DEFROST = 12"]},{"cell_type":"code","execution_count":null,"id":"21e7a400-b99f-4858-b25b-35146b8e1927","metadata":{"id":"21e7a400-b99f-4858-b25b-35146b8e1927"},"outputs":[],"source":["model_vgg16 = keras.models.load_model(MODELS_DIR_VGG / '03_finetuning_pass2')\n","\n","model_vgg16.get_layer('vgg16').trainable = True\n","for i, layer in enumerate(model_vgg16.get_layer('vgg16').layers[:-DEFROST]):\n","    layer.trainable = False\n","\n","compile_model(model_vgg16, lr=1e-5, opt='Adam')"]},{"cell_type":"code","execution_count":null,"id":"89c61645-6897-49e8-a208-3389a2d9cd64","metadata":{"id":"89c61645-6897-49e8-a208-3389a2d9cd64"},"outputs":[],"source":["append_callback = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-6)\n","history = fit_model(model_vgg16, patience=30, append_callback=append_callback)\n","evaluate_model(model_vgg16, validation_dataset)\n","nice_plot(history)"]},{"cell_type":"code","execution_count":null,"id":"67db56c4-8a52-43e2-882d-e1d9bd1aecb7","metadata":{"id":"67db56c4-8a52-43e2-882d-e1d9bd1aecb7"},"outputs":[],"source":["model_vgg16.save(MODELS_DIR_VGG / '04_finetuning_pass3')"]},{"cell_type":"markdown","id":"086a0dc4-6e11-4d0d-b1e8-33902018cdac","metadata":{"id":"086a0dc4-6e11-4d0d-b1e8-33902018cdac"},"source":["# Training ResNet-50"]},{"cell_type":"code","execution_count":null,"id":"2067ff05-87fc-45c5-9210-1be0a90feb13","metadata":{"id":"2067ff05-87fc-45c5-9210-1be0a90feb13"},"outputs":[],"source":["MODELS_DIR_RESNET = MODELS_DIR / 'model_resnet50'\n","MODELS_DIR_RESNET.mkdir(parents=True, exist_ok=True)"]},{"cell_type":"markdown","id":"776ad437-dbb2-4889-a58a-bfb148757ecd","metadata":{"id":"776ad437-dbb2-4889-a58a-bfb148757ecd"},"source":["## Instantiate supernet"]},{"cell_type":"code","execution_count":null,"id":"261f3e11-7370-433d-bcd0-c5900c331c73","metadata":{"id":"261f3e11-7370-433d-bcd0-c5900c331c73"},"outputs":[],"source":["conv_base  = keras.applications.ResNet50(\n","    weights='imagenet',\n","    include_top=False,\n","    input_shape=(224, 224,3)\n",")\n","conv_base.trainable = False"]},{"cell_type":"markdown","id":"add624f2-d5e9-4683-9fe1-c774719fb1b0","metadata":{"id":"add624f2-d5e9-4683-9fe1-c774719fb1b0"},"source":["## Build model"]},{"cell_type":"code","execution_count":null,"id":"7f8f7ca2-282f-4acb-be07-93e25de48a1d","metadata":{"id":"7f8f7ca2-282f-4acb-be07-93e25de48a1d"},"outputs":[],"source":["inputs = keras.Input(shape=INPUT_SHAPE)\n","\n","x = layers.Resizing(224, 224, interpolation='bicubic', name='resizing')(inputs)\n","x = keras.applications.resnet.preprocess_input(x)\n","x = conv_base(x)\n","\n","x = layers.GlobalAveragePooling2D()(x)\n","\n","x = layers.Dense(512, kernel_initializer=keras.initializers.HeUniform(seed=SEED))(x)\n","x = layers.BatchNormalization()(x)\n","x = layers.Activation('relu')(x)\n","x = layers.Dropout(0.5)(x)\n","\n","outputs = layers.Dense(NUM_CLASSES, activation='softmax', kernel_initializer=keras.initializers.GlorotUniform(seed=SEED))(x)\n","\n","model_resnet50 = keras.Model(inputs, outputs, name='resnet50')\n","\n","compile_model(model_resnet50, lr=1e-3, opt='Adam')"]},{"cell_type":"markdown","id":"6d3880a5-878d-42d1-8b03-0959c6d228d5","metadata":{"id":"6d3880a5-878d-42d1-8b03-0959c6d228d5"},"source":["## Perform transfer learning"]},{"cell_type":"code","execution_count":null,"id":"193aac95-ca0d-45fe-ae72-ae947b967c01","metadata":{"id":"193aac95-ca0d-45fe-ae72-ae947b967c01"},"outputs":[],"source":["history = fit_model(model_resnet50, patience=20, weights=class_weights)\n","evaluate_model(model_resnet50, validation_dataset)\n","nice_plot(history)"]},{"cell_type":"code","execution_count":null,"id":"f99871c3-ee04-4925-8adb-a76a80665b94","metadata":{"id":"f99871c3-ee04-4925-8adb-a76a80665b94"},"outputs":[],"source":["model_resnet50.save(MODELS_DIR_RESNET / '01_no_finetuning')"]},{"cell_type":"markdown","id":"70c1cd71-c946-418d-bd73-8ae98d15e623","metadata":{"id":"70c1cd71-c946-418d-bd73-8ae98d15e623"},"source":["## Fine tuning (1st pass)"]},{"cell_type":"code","execution_count":null,"id":"45593e01-e98a-410f-873e-0209e6c6454b","metadata":{"id":"45593e01-e98a-410f-873e-0209e6c6454b"},"outputs":[],"source":["DEFROST = 32"]},{"cell_type":"code","execution_count":null,"id":"15ab84d0-bb01-4a18-9b86-5f93f65058cd","metadata":{"id":"15ab84d0-bb01-4a18-9b86-5f93f65058cd"},"outputs":[],"source":["model_resnet50 = keras.models.load_model(MODELS_DIR_RESNET / '01_no_finetuning')\n","\n","model_resnet50.get_layer('resnet50').trainable = True\n","for i, layer in enumerate(model_resnet50.get_layer('resnet50').layers[:-DEFROST]):\n","    layer.trainable = False\n","\n","# make sure BatchNorm layers are frozen\n","for i, layer in enumerate(model_resnet50.get_layer('resnet50').layers):\n","    if isinstance(layer, layers.BatchNormalization):\n","        layer.trainable = False\n","\n","compile_model(model_resnet50, lr=1e-4, opt='Adam')"]},{"cell_type":"code","execution_count":null,"id":"9302718b-e96b-49f5-bf94-093949865e48","metadata":{"id":"9302718b-e96b-49f5-bf94-093949865e48"},"outputs":[],"source":["append_callback = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-6)\n","history = fit_model(model_resnet50, patience=20, append_callback=append_callback)\n","evaluate_model(model_resnet50, validation_dataset)\n","nice_plot(history)"]},{"cell_type":"code","execution_count":null,"id":"e53eb23f-23ad-424b-bfb8-c1985b894553","metadata":{"id":"e53eb23f-23ad-424b-bfb8-c1985b894553"},"outputs":[],"source":["model_resnet50.save(MODELS_DIR_RESNET / '02_finetuning_pass1')"]},{"cell_type":"markdown","id":"c037bbdd-8749-4d34-aab4-6bedecae893c","metadata":{"id":"c037bbdd-8749-4d34-aab4-6bedecae893c"},"source":["## Fine tuning (2nd pass)"]},{"cell_type":"code","execution_count":null,"id":"bd88e845-332e-4786-b033-2a45e35e27a9","metadata":{"id":"bd88e845-332e-4786-b033-2a45e35e27a9"},"outputs":[],"source":["DEFROST = 64"]},{"cell_type":"code","execution_count":null,"id":"af30ef86-032d-48cd-b3c7-f5088303cfff","metadata":{"id":"af30ef86-032d-48cd-b3c7-f5088303cfff"},"outputs":[],"source":["model_resnet50 = keras.models.load_model(MODELS_DIR_RESNET / '02_finetuning_pass1')\n","\n","model_resnet50.get_layer('resnet50').trainable = True\n","for i, layer in enumerate(model_resnet50.get_layer('resnet50').layers[:-DEFROST]):\n","    layer.trainable = False\n","\n","# make sure BatchNorm layers are frozen\n","for i, layer in enumerate(model_resnet50.get_layer('resnet50').layers):\n","    if isinstance(layer, layers.BatchNormalization):\n","        layer.trainable = False\n","\n","compile_model(model_resnet50, lr=1e-4, opt='Adam')"]},{"cell_type":"code","execution_count":null,"id":"047fc4e9-e1c6-41a3-9972-126a80f6b9c6","metadata":{"id":"047fc4e9-e1c6-41a3-9972-126a80f6b9c6"},"outputs":[],"source":["append_callback = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=15, min_lr=1e-6)\n","history = fit_model(model_resnet50, patience=20, append_callback=append_callback)\n","evaluate_model(model_resnet50, validation_dataset)\n","nice_plot(history)"]},{"cell_type":"code","execution_count":null,"id":"74fc8a79-1c8d-4860-a9ee-5c01d917e7b4","metadata":{"id":"74fc8a79-1c8d-4860-a9ee-5c01d917e7b4"},"outputs":[],"source":["model_resnet50.save(MODELS_DIR_RESNET / '03_finetuning_pass2')"]},{"cell_type":"markdown","id":"ff70ed5e-807b-45f4-92c6-544f0b43d894","metadata":{"id":"ff70ed5e-807b-45f4-92c6-544f0b43d894"},"source":["## Fine tuning (3rd pass)"]},{"cell_type":"code","execution_count":null,"id":"30ac9e6a-4a0c-4aa4-938f-638d62b17cd4","metadata":{"id":"30ac9e6a-4a0c-4aa4-938f-638d62b17cd4"},"outputs":[],"source":["DEFROST = 96"]},{"cell_type":"code","execution_count":null,"id":"a171a40c-f438-417a-b47f-4e4e470d57b7","metadata":{"id":"a171a40c-f438-417a-b47f-4e4e470d57b7"},"outputs":[],"source":["model_resnet50 = keras.models.load_model(MODELS_DIR_RESNET / '03_finetuning_pass2')\n","\n","model_resnet50.get_layer('resnet50').trainable = True\n","for i, layer in enumerate(model_resnet50.get_layer('resnet50').layers[:-DEFROST]):\n","    layer.trainable = False\n","\n","# make sure BatchNorm layers are frozen\n","for i, layer in enumerate(model_resnet50.get_layer('resnet50').layers):\n","    if isinstance(layer, layers.BatchNormalization):\n","        layer.trainable = False\n","\n","compile_model(model_resnet50, lr=1e-5, opt='Adam')"]},{"cell_type":"code","execution_count":null,"id":"842ff061-c30e-4680-b160-73fd7ae9445e","metadata":{"id":"842ff061-c30e-4680-b160-73fd7ae9445e"},"outputs":[],"source":["append_callback = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=15, min_lr=1e-7)\n","history = fit_model(model_resnet50, patience=30, append_callback=append_callback)\n","evaluate_model(model_resnet50, validation_dataset)\n","nice_plot(history)"]},{"cell_type":"code","execution_count":null,"id":"d6b5f200-1dfd-48ce-ab7a-69604030f724","metadata":{"id":"d6b5f200-1dfd-48ce-ab7a-69604030f724"},"outputs":[],"source":["model_resnet50.save(MODELS_DIR_RESNET / '04_finetuning_pass3')"]},{"cell_type":"markdown","id":"5404fd9e-60de-4e23-b7f5-2fc9ebbaca3e","metadata":{"id":"5404fd9e-60de-4e23-b7f5-2fc9ebbaca3e"},"source":["# Building the ensemble"]},{"cell_type":"markdown","id":"0a107786-a81d-486b-ad66-e6c46a132b40","metadata":{"id":"0a107786-a81d-486b-ad66-e6c46a132b40"},"source":["Choose the best models from the previous trainings and put them together.\n","\n","**Important note:** due to stochasticity, if the notebook if re-run it may happen that a different unfreeze configuration performs better. In such case, adjust manually here to build the ensemble from the two best models of the supernets."]},{"cell_type":"code","execution_count":null,"id":"1b453225-0d1f-4919-9c09-6955a1adcd8b","metadata":{"id":"1b453225-0d1f-4919-9c09-6955a1adcd8b"},"outputs":[],"source":["model_vgg16 = keras.models.load_model(MODELS_DIR_VGG / '04_finetuning_pass3')\n","model_resnet50 = keras.models.load_model(MODELS_DIR_RESNET / '03_finetuning_pass2')\n","models = [model_vgg16, model_resnet50]"]},{"cell_type":"markdown","id":"3ef01d0a-c8cf-465a-a6ec-5711fda0df7a","metadata":{"id":"3ef01d0a-c8cf-465a-a6ec-5711fda0df7a"},"source":["[On Stack Overflow](https://stackoverflow.com/questions/67647843/is-there-a-way-to-ensemble-two-keras-h5-models-trained-for-same-classes) there is also specified a way to weigh them differently."]},{"cell_type":"code","execution_count":null,"id":"4659a831-54b0-4602-ab19-8a1267d74c14","metadata":{"id":"4659a831-54b0-4602-ab19-8a1267d74c14"},"outputs":[],"source":["inputs = keras.Input(shape=INPUT_SHAPE)\n","outputs = [model(inputs) for model in models]\n","outputs_ensemble = layers.Average()(outputs)\n","model_ensemble = keras.Model(inputs=inputs, outputs=outputs_ensemble, name='ensemble_vgg16-resnet50')\n","model_ensemble.save(MODELS_DIR / 'ensemble_vgg16-resnet50')\n","# keras.utils.plot_model(ensemble_model, to_file=str(Path() / 'report_material' / 'ensemble_vgg16-resnet50.png')"]},{"cell_type":"markdown","id":"2f97cb92-3946-4a91-86e1-4ccb7fe46c72","metadata":{"id":"2f97cb92-3946-4a91-86e1-4ccb7fe46c72"},"source":["# Implementing Test-Time Augmentation\n","\n","The source code of the `model.py` to implement TTA is as follows:"]},{"cell_type":"code","execution_count":null,"id":"b92ce323-077c-4321-bef0-58181c6c0793","metadata":{"id":"b92ce323-077c-4321-bef0-58181c6c0793"},"outputs":[],"source":["import os\n","import tensorflow as tf\n","import numpy as np\n","\n","class model:\n","    def __init__(self, path):\n","        self.model = tf.keras.models.load_model(os.path.join(path, 'ensemble_vgg16-resnet50'))\n","        \n","    def predict(self, X):\n","\n","        PERFORM_TTA = False\n","        \n","        if PERFORM_TTA:\n","\n","            TTA_STEPS = 10\n","            BATCH_SIZE = 32\n","\n","            test_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n","                rotation_range=40,\n","                width_shift_range=0.2,\n","                height_shift_range=0.2,\n","                zoom_range=[0.5,1.5],\n","                brightness_range=[0.5,1.5],\n","                shear_range=0.2,\n","                vertical_flip=True,\n","                horizontal_flip=True,\n","                fill_mode='reflect')\n","\n","            # make predictions\n","            yhats = []\n","            \n","            for i in range(TTA_STEPS):\n","                # predict augmented test set\n","                # preds = self.model.predict(test_gen.flow(X, batch_size=BATCH_SIZE, shuffle=False)) # version with the new .predict()\n","                preds = self.model.predict_generator(test_gen.flow(X, batch_size=BATCH_SIZE, shuffle=False), steps=len(X)/BATCH_SIZE)\n","                yhats.append(preds)\n","\n","            yhats = np.array(yhats)\n","            yhats = np.mean(yhats, axis=0) # take average of predictions of augmented images\n","            yhats = np.argmax(yhats, axis=1) # argmax across classes\n","\n","        else:\n","\n","            yhats = self.model.predict(X)\n","            yhats = tf.argmax(yhats, axis=1) # argmax across classes\n","\n","        return yhats"]},{"cell_type":"markdown","source":["# Computing metrics\n","\n","In the separate `metrics.ipynb` notebook for clarity."],"metadata":{"id":"7BEF6VrqUxTZ"},"id":"7BEF6VrqUxTZ"}],"metadata":{"kernelspec":{"display_name":"TensorFlow","language":"python","name":"tf"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"colab":{"provenance":[],"collapsed_sections":["f37a0700-5916-49f1-ae0e-fced4fd92ba1","9038d9fd-7221-4e85-9dc2-114413aea402","e6ef3e22-3ada-4741-8439-69c91d5ffd35","916a9ef5-d6d9-4d63-9b5f-ec8ed29c5ee9","05bef5ca-4b2e-4c7e-b5b8-4bbcc13f606f","776ad437-dbb2-4889-a58a-bfb148757ecd","add624f2-d5e9-4683-9fe1-c774719fb1b0","6d3880a5-878d-42d1-8b03-0959c6d228d5","70c1cd71-c946-418d-bd73-8ae98d15e623","c037bbdd-8749-4d34-aab4-6bedecae893c"]}},"nbformat":4,"nbformat_minor":5}